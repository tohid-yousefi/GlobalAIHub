{"cells":[{"cell_type":"markdown","source":["# Classification - GlobalAIHub\n","\n","üìå **Classification** is the process of categorizing a given set of data into classes. Here, the classes act as our labels, or ground truth. A classification model uses the features of an object to predict its labels. As we say labels, you can probably guess that here we have another **supervised learning** model at hand. The algorithm used by your email service providers to filter spam from non-spam emails is an example of classification. This model uses the features of the email: subject, sender‚Äôs email address, email body, and attachments as inputs; and makes a prediction for one out of the two classes: spam or non-spam. This is an example of *binary classification*, where the output is restricted to two classes. Spam and non-spam, true and false, zeros and ones, yes and no, positive, or negative and so on. If there are more than two classes, we have a **multi-class classification** problem. An example of multi-class classification can be classifying types of fruits based on their color, weight, and size. Or movies into different genres like comedy, romance, drama, and horror.\n","\n","üìå The question is, how can machine learning solve this problem? Let‚Äôs start with our first classification model: **Logistic regression**. The best way to think about logistic regression is that it is a *linear regression* but for classification problems. Logistic regression uses a logistic function, specifically the **Sigmoid function**. This function takes any real input, and outputs a value between zero and one. Unlike linear regression, logistic regression doesn‚Äôt need a linear relationship between input and output variables. Once we have the predicted results from our classification model, or classifier, we compare these results with the actual label, the ground truth, and evaluate the performance of our model."],"metadata":{"id":"W13hDBlCan8p"},"id":"W13hDBlCan8p"},{"cell_type":"markdown","source":["üìå We have a binary classification problem. We need to classify tumors into malignant ones that are cancerous or benign which means non-cancerous. Our dataset contains statistical data from histopathology examinations. We will use this dataset to train our logistic regression model. Let‚Äôs import pandas and read the dataset. Using the shape method, we can easily get the number of observations and features. In this dataset, there are unique instances. The features are radius mean, texture mean, radius worst, and so on. There are features in total. We can check the first 5 instances in our dataset using the head function. Here, we can see, the first column represents the target variable. The following columns are the features."],"metadata":{"id":"qM7egIhVb4pK"},"id":"qM7egIhVb4pK"},{"cell_type":"code","execution_count":null,"id":"6a2bceed","metadata":{"id":"6a2bceed"},"outputs":[],"source":["import pandas as pd\n","dataset = pd.read_csv(\"breast-cancer.csv\")"]},{"cell_type":"code","execution_count":null,"id":"bf1c090b","metadata":{"id":"bf1c090b","outputId":"522acbe6-b426-45f5-c188-c272df399b7f"},"outputs":[{"data":{"text/plain":["(569, 31)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["dataset.shape"]},{"cell_type":"code","execution_count":null,"id":"272cd872","metadata":{"id":"272cd872","outputId":"8a6f9bfa-0336-4380-8a01-d0f506b70000"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>...</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>M</td>\n","      <td>1.097064</td>\n","      <td>-2.073335</td>\n","      <td>1.269934</td>\n","      <td>0.984375</td>\n","      <td>1.568466</td>\n","      <td>3.283515</td>\n","      <td>2.652874</td>\n","      <td>2.532475</td>\n","      <td>2.217515</td>\n","      <td>...</td>\n","      <td>1.886690</td>\n","      <td>-1.359293</td>\n","      <td>2.303601</td>\n","      <td>2.001237</td>\n","      <td>1.307686</td>\n","      <td>2.616665</td>\n","      <td>2.109526</td>\n","      <td>2.296076</td>\n","      <td>2.750622</td>\n","      <td>1.937015</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>M</td>\n","      <td>1.829821</td>\n","      <td>-0.353632</td>\n","      <td>1.685955</td>\n","      <td>1.908708</td>\n","      <td>-0.826962</td>\n","      <td>-0.487072</td>\n","      <td>-0.023846</td>\n","      <td>0.548144</td>\n","      <td>0.001392</td>\n","      <td>...</td>\n","      <td>1.805927</td>\n","      <td>-0.369203</td>\n","      <td>1.535126</td>\n","      <td>1.890489</td>\n","      <td>-0.375612</td>\n","      <td>-0.430444</td>\n","      <td>-0.146749</td>\n","      <td>1.087084</td>\n","      <td>-0.243890</td>\n","      <td>0.281190</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>M</td>\n","      <td>1.579888</td>\n","      <td>0.456187</td>\n","      <td>1.566503</td>\n","      <td>1.558884</td>\n","      <td>0.942210</td>\n","      <td>1.052926</td>\n","      <td>1.363478</td>\n","      <td>2.037231</td>\n","      <td>0.939685</td>\n","      <td>...</td>\n","      <td>1.511870</td>\n","      <td>-0.023974</td>\n","      <td>1.347475</td>\n","      <td>1.456285</td>\n","      <td>0.527407</td>\n","      <td>1.082932</td>\n","      <td>0.854974</td>\n","      <td>1.955000</td>\n","      <td>1.152255</td>\n","      <td>0.201391</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>M</td>\n","      <td>-0.768909</td>\n","      <td>0.253732</td>\n","      <td>-0.592687</td>\n","      <td>-0.764464</td>\n","      <td>3.283553</td>\n","      <td>3.402909</td>\n","      <td>1.915897</td>\n","      <td>1.451707</td>\n","      <td>2.867383</td>\n","      <td>...</td>\n","      <td>-0.281464</td>\n","      <td>0.133984</td>\n","      <td>-0.249939</td>\n","      <td>-0.550021</td>\n","      <td>3.394275</td>\n","      <td>3.893397</td>\n","      <td>1.989588</td>\n","      <td>2.175786</td>\n","      <td>6.046041</td>\n","      <td>4.935010</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>M</td>\n","      <td>1.750297</td>\n","      <td>-1.151816</td>\n","      <td>1.776573</td>\n","      <td>1.826229</td>\n","      <td>0.280372</td>\n","      <td>0.539340</td>\n","      <td>1.371011</td>\n","      <td>1.428493</td>\n","      <td>-0.009560</td>\n","      <td>...</td>\n","      <td>1.298575</td>\n","      <td>-1.466770</td>\n","      <td>1.338539</td>\n","      <td>1.220724</td>\n","      <td>0.220556</td>\n","      <td>-0.313395</td>\n","      <td>0.613179</td>\n","      <td>0.729259</td>\n","      <td>-0.868353</td>\n","      <td>-0.397100</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows √ó 31 columns</p>\n","</div>"],"text/plain":["  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","0         M     1.097064     -2.073335        1.269934   0.984375   \n","1         M     1.829821     -0.353632        1.685955   1.908708   \n","2         M     1.579888      0.456187        1.566503   1.558884   \n","3         M    -0.768909      0.253732       -0.592687  -0.764464   \n","4         M     1.750297     -1.151816        1.776573   1.826229   \n","\n","   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","0         1.568466          3.283515        2.652874             2.532475   \n","1        -0.826962         -0.487072       -0.023846             0.548144   \n","2         0.942210          1.052926        1.363478             2.037231   \n","3         3.283553          3.402909        1.915897             1.451707   \n","4         0.280372          0.539340        1.371011             1.428493   \n","\n","   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n","0       2.217515  ...      1.886690      -1.359293         2.303601   \n","1       0.001392  ...      1.805927      -0.369203         1.535126   \n","2       0.939685  ...      1.511870      -0.023974         1.347475   \n","3       2.867383  ...     -0.281464       0.133984        -0.249939   \n","4      -0.009560  ...      1.298575      -1.466770         1.338539   \n","\n","   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n","0    2.001237          1.307686           2.616665         2.109526   \n","1    1.890489         -0.375612          -0.430444        -0.146749   \n","2    1.456285          0.527407           1.082932         0.854974   \n","3   -0.550021          3.394275           3.893397         1.989588   \n","4    1.220724          0.220556          -0.313395         0.613179   \n","\n","   concave points_worst  symmetry_worst  fractal_dimension_worst  \n","0              2.296076        2.750622                 1.937015  \n","1              1.087084       -0.243890                 0.281190  \n","2              1.955000        1.152255                 0.201391  \n","3              2.175786        6.046041                 4.935010  \n","4              0.729259       -0.868353                -0.397100  \n","\n","[5 rows x 31 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset.head()"]},{"cell_type":"code","execution_count":null,"id":"395ced3c","metadata":{"scrolled":true,"id":"395ced3c","outputId":"1e2dca05-d8a3-4467-f2ff-7bd31c56a648"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>...</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>564</th>\n","      <td>M</td>\n","      <td>2.110995</td>\n","      <td>0.721473</td>\n","      <td>2.060786</td>\n","      <td>2.343856</td>\n","      <td>1.041842</td>\n","      <td>0.219060</td>\n","      <td>1.947285</td>\n","      <td>2.320965</td>\n","      <td>-0.312589</td>\n","      <td>...</td>\n","      <td>1.901185</td>\n","      <td>0.117700</td>\n","      <td>1.752563</td>\n","      <td>2.015301</td>\n","      <td>0.378365</td>\n","      <td>-0.273318</td>\n","      <td>0.664512</td>\n","      <td>1.629151</td>\n","      <td>-1.360158</td>\n","      <td>-0.709091</td>\n","    </tr>\n","    <tr>\n","      <th>565</th>\n","      <td>M</td>\n","      <td>1.704854</td>\n","      <td>2.085134</td>\n","      <td>1.615931</td>\n","      <td>1.723842</td>\n","      <td>0.102458</td>\n","      <td>-0.017833</td>\n","      <td>0.693043</td>\n","      <td>1.263669</td>\n","      <td>-0.217664</td>\n","      <td>...</td>\n","      <td>1.536720</td>\n","      <td>2.047399</td>\n","      <td>1.421940</td>\n","      <td>1.494959</td>\n","      <td>-0.691230</td>\n","      <td>-0.394820</td>\n","      <td>0.236573</td>\n","      <td>0.733827</td>\n","      <td>-0.531855</td>\n","      <td>-0.973978</td>\n","    </tr>\n","    <tr>\n","      <th>566</th>\n","      <td>M</td>\n","      <td>0.702284</td>\n","      <td>2.045574</td>\n","      <td>0.672676</td>\n","      <td>0.577953</td>\n","      <td>-0.840484</td>\n","      <td>-0.038680</td>\n","      <td>0.046588</td>\n","      <td>0.105777</td>\n","      <td>-0.809117</td>\n","      <td>...</td>\n","      <td>0.561361</td>\n","      <td>1.374854</td>\n","      <td>0.579001</td>\n","      <td>0.427906</td>\n","      <td>-0.809587</td>\n","      <td>0.350735</td>\n","      <td>0.326767</td>\n","      <td>0.414069</td>\n","      <td>-1.104549</td>\n","      <td>-0.318409</td>\n","    </tr>\n","    <tr>\n","      <th>567</th>\n","      <td>M</td>\n","      <td>1.838341</td>\n","      <td>2.336457</td>\n","      <td>1.982524</td>\n","      <td>1.735218</td>\n","      <td>1.525767</td>\n","      <td>3.272144</td>\n","      <td>3.296944</td>\n","      <td>2.658866</td>\n","      <td>2.137194</td>\n","      <td>...</td>\n","      <td>1.961239</td>\n","      <td>2.237926</td>\n","      <td>2.303601</td>\n","      <td>1.653171</td>\n","      <td>1.430427</td>\n","      <td>3.904848</td>\n","      <td>3.197605</td>\n","      <td>2.289985</td>\n","      <td>1.919083</td>\n","      <td>2.219635</td>\n","    </tr>\n","    <tr>\n","      <th>568</th>\n","      <td>B</td>\n","      <td>-1.808401</td>\n","      <td>1.221792</td>\n","      <td>-1.814389</td>\n","      <td>-1.347789</td>\n","      <td>-3.112085</td>\n","      <td>-1.150752</td>\n","      <td>-1.114873</td>\n","      <td>-1.261820</td>\n","      <td>-0.820070</td>\n","      <td>...</td>\n","      <td>-1.410893</td>\n","      <td>0.764190</td>\n","      <td>-1.432735</td>\n","      <td>-1.075813</td>\n","      <td>-1.859019</td>\n","      <td>-1.207552</td>\n","      <td>-1.305831</td>\n","      <td>-1.745063</td>\n","      <td>-0.048138</td>\n","      <td>-0.751207</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows √ó 31 columns</p>\n","</div>"],"text/plain":["    diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","564         M     2.110995      0.721473        2.060786   2.343856   \n","565         M     1.704854      2.085134        1.615931   1.723842   \n","566         M     0.702284      2.045574        0.672676   0.577953   \n","567         M     1.838341      2.336457        1.982524   1.735218   \n","568         B    -1.808401      1.221792       -1.814389  -1.347789   \n","\n","     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","564         1.041842          0.219060        1.947285             2.320965   \n","565         0.102458         -0.017833        0.693043             1.263669   \n","566        -0.840484         -0.038680        0.046588             0.105777   \n","567         1.525767          3.272144        3.296944             2.658866   \n","568        -3.112085         -1.150752       -1.114873            -1.261820   \n","\n","     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n","564      -0.312589  ...      1.901185       0.117700         1.752563   \n","565      -0.217664  ...      1.536720       2.047399         1.421940   \n","566      -0.809117  ...      0.561361       1.374854         0.579001   \n","567       2.137194  ...      1.961239       2.237926         2.303601   \n","568      -0.820070  ...     -1.410893       0.764190        -1.432735   \n","\n","     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n","564    2.015301          0.378365          -0.273318         0.664512   \n","565    1.494959         -0.691230          -0.394820         0.236573   \n","566    0.427906         -0.809587           0.350735         0.326767   \n","567    1.653171          1.430427           3.904848         3.197605   \n","568   -1.075813         -1.859019          -1.207552        -1.305831   \n","\n","     concave points_worst  symmetry_worst  fractal_dimension_worst  \n","564              1.629151       -1.360158                -0.709091  \n","565              0.733827       -0.531855                -0.973978  \n","566              0.414069       -1.104549                -0.318409  \n","567              2.289985        1.919083                 2.219635  \n","568             -1.745063       -0.048138                -0.751207  \n","\n","[5 rows x 31 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["dataset.tail()"]},{"cell_type":"markdown","source":["üìå This is a real-life dataset and before we can apply machine learning algorithms to it, it has to be cleaned and organized. Since we know that machines operate with numbers, we need to convert our target variable from categorical to numerical type. There are many ways to do this. One of the simpler methods is called **label encoding**. With this, we can convert ‚ÄúM‚Äù and ‚ÄúB‚Äù to 1 and 0. As a first step, we import **LabelEncoder** from **sklearn** library. Then, to make it easier to use, we assign LabelEncoder to the ‚Äúlabelencoder‚Äù variable. Finally, we convert the ‚Äúdiagnosis‚Äù column from categoric to numeric with the code in the third line."],"metadata":{"id":"wWQ3Q4AucJmK"},"id":"wWQ3Q4AucJmK"},{"cell_type":"code","execution_count":null,"id":"2e583520","metadata":{"id":"2e583520"},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","labelencoder = LabelEncoder()\n","dataset[\"diagnosis\"] = labelencoder.fit_transform(dataset[\"diagnosis\"].values) "]},{"cell_type":"code","execution_count":null,"id":"e25a66ff","metadata":{"id":"e25a66ff","outputId":"31d4a744-1316-46eb-8463-10292520b376"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>...</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1.097064</td>\n","      <td>-2.073335</td>\n","      <td>1.269934</td>\n","      <td>0.984375</td>\n","      <td>1.568466</td>\n","      <td>3.283515</td>\n","      <td>2.652874</td>\n","      <td>2.532475</td>\n","      <td>2.217515</td>\n","      <td>...</td>\n","      <td>1.886690</td>\n","      <td>-1.359293</td>\n","      <td>2.303601</td>\n","      <td>2.001237</td>\n","      <td>1.307686</td>\n","      <td>2.616665</td>\n","      <td>2.109526</td>\n","      <td>2.296076</td>\n","      <td>2.750622</td>\n","      <td>1.937015</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1.829821</td>\n","      <td>-0.353632</td>\n","      <td>1.685955</td>\n","      <td>1.908708</td>\n","      <td>-0.826962</td>\n","      <td>-0.487072</td>\n","      <td>-0.023846</td>\n","      <td>0.548144</td>\n","      <td>0.001392</td>\n","      <td>...</td>\n","      <td>1.805927</td>\n","      <td>-0.369203</td>\n","      <td>1.535126</td>\n","      <td>1.890489</td>\n","      <td>-0.375612</td>\n","      <td>-0.430444</td>\n","      <td>-0.146749</td>\n","      <td>1.087084</td>\n","      <td>-0.243890</td>\n","      <td>0.281190</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>1.579888</td>\n","      <td>0.456187</td>\n","      <td>1.566503</td>\n","      <td>1.558884</td>\n","      <td>0.942210</td>\n","      <td>1.052926</td>\n","      <td>1.363478</td>\n","      <td>2.037231</td>\n","      <td>0.939685</td>\n","      <td>...</td>\n","      <td>1.511870</td>\n","      <td>-0.023974</td>\n","      <td>1.347475</td>\n","      <td>1.456285</td>\n","      <td>0.527407</td>\n","      <td>1.082932</td>\n","      <td>0.854974</td>\n","      <td>1.955000</td>\n","      <td>1.152255</td>\n","      <td>0.201391</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>-0.768909</td>\n","      <td>0.253732</td>\n","      <td>-0.592687</td>\n","      <td>-0.764464</td>\n","      <td>3.283553</td>\n","      <td>3.402909</td>\n","      <td>1.915897</td>\n","      <td>1.451707</td>\n","      <td>2.867383</td>\n","      <td>...</td>\n","      <td>-0.281464</td>\n","      <td>0.133984</td>\n","      <td>-0.249939</td>\n","      <td>-0.550021</td>\n","      <td>3.394275</td>\n","      <td>3.893397</td>\n","      <td>1.989588</td>\n","      <td>2.175786</td>\n","      <td>6.046041</td>\n","      <td>4.935010</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>1.750297</td>\n","      <td>-1.151816</td>\n","      <td>1.776573</td>\n","      <td>1.826229</td>\n","      <td>0.280372</td>\n","      <td>0.539340</td>\n","      <td>1.371011</td>\n","      <td>1.428493</td>\n","      <td>-0.009560</td>\n","      <td>...</td>\n","      <td>1.298575</td>\n","      <td>-1.466770</td>\n","      <td>1.338539</td>\n","      <td>1.220724</td>\n","      <td>0.220556</td>\n","      <td>-0.313395</td>\n","      <td>0.613179</td>\n","      <td>0.729259</td>\n","      <td>-0.868353</td>\n","      <td>-0.397100</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows √ó 31 columns</p>\n","</div>"],"text/plain":["   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","0          1     1.097064     -2.073335        1.269934   0.984375   \n","1          1     1.829821     -0.353632        1.685955   1.908708   \n","2          1     1.579888      0.456187        1.566503   1.558884   \n","3          1    -0.768909      0.253732       -0.592687  -0.764464   \n","4          1     1.750297     -1.151816        1.776573   1.826229   \n","\n","   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","0         1.568466          3.283515        2.652874             2.532475   \n","1        -0.826962         -0.487072       -0.023846             0.548144   \n","2         0.942210          1.052926        1.363478             2.037231   \n","3         3.283553          3.402909        1.915897             1.451707   \n","4         0.280372          0.539340        1.371011             1.428493   \n","\n","   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n","0       2.217515  ...      1.886690      -1.359293         2.303601   \n","1       0.001392  ...      1.805927      -0.369203         1.535126   \n","2       0.939685  ...      1.511870      -0.023974         1.347475   \n","3       2.867383  ...     -0.281464       0.133984        -0.249939   \n","4      -0.009560  ...      1.298575      -1.466770         1.338539   \n","\n","   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n","0    2.001237          1.307686           2.616665         2.109526   \n","1    1.890489         -0.375612          -0.430444        -0.146749   \n","2    1.456285          0.527407           1.082932         0.854974   \n","3   -0.550021          3.394275           3.893397         1.989588   \n","4    1.220724          0.220556          -0.313395         0.613179   \n","\n","   concave points_worst  symmetry_worst  fractal_dimension_worst  \n","0              2.296076        2.750622                 1.937015  \n","1              1.087084       -0.243890                 0.281190  \n","2              1.955000        1.152255                 0.201391  \n","3              2.175786        6.046041                 4.935010  \n","4              0.729259       -0.868353                -0.397100  \n","\n","[5 rows x 31 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dataset.head()"]},{"cell_type":"code","execution_count":null,"id":"40839716","metadata":{"id":"40839716","outputId":"5eedb3fc-8126-4a1f-cd0a-b4d430e441ed"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>diagnosis</th>\n","      <th>radius_mean</th>\n","      <th>texture_mean</th>\n","      <th>perimeter_mean</th>\n","      <th>area_mean</th>\n","      <th>smoothness_mean</th>\n","      <th>compactness_mean</th>\n","      <th>concavity_mean</th>\n","      <th>concave points_mean</th>\n","      <th>symmetry_mean</th>\n","      <th>...</th>\n","      <th>radius_worst</th>\n","      <th>texture_worst</th>\n","      <th>perimeter_worst</th>\n","      <th>area_worst</th>\n","      <th>smoothness_worst</th>\n","      <th>compactness_worst</th>\n","      <th>concavity_worst</th>\n","      <th>concave points_worst</th>\n","      <th>symmetry_worst</th>\n","      <th>fractal_dimension_worst</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>564</th>\n","      <td>1</td>\n","      <td>2.110995</td>\n","      <td>0.721473</td>\n","      <td>2.060786</td>\n","      <td>2.343856</td>\n","      <td>1.041842</td>\n","      <td>0.219060</td>\n","      <td>1.947285</td>\n","      <td>2.320965</td>\n","      <td>-0.312589</td>\n","      <td>...</td>\n","      <td>1.901185</td>\n","      <td>0.117700</td>\n","      <td>1.752563</td>\n","      <td>2.015301</td>\n","      <td>0.378365</td>\n","      <td>-0.273318</td>\n","      <td>0.664512</td>\n","      <td>1.629151</td>\n","      <td>-1.360158</td>\n","      <td>-0.709091</td>\n","    </tr>\n","    <tr>\n","      <th>565</th>\n","      <td>1</td>\n","      <td>1.704854</td>\n","      <td>2.085134</td>\n","      <td>1.615931</td>\n","      <td>1.723842</td>\n","      <td>0.102458</td>\n","      <td>-0.017833</td>\n","      <td>0.693043</td>\n","      <td>1.263669</td>\n","      <td>-0.217664</td>\n","      <td>...</td>\n","      <td>1.536720</td>\n","      <td>2.047399</td>\n","      <td>1.421940</td>\n","      <td>1.494959</td>\n","      <td>-0.691230</td>\n","      <td>-0.394820</td>\n","      <td>0.236573</td>\n","      <td>0.733827</td>\n","      <td>-0.531855</td>\n","      <td>-0.973978</td>\n","    </tr>\n","    <tr>\n","      <th>566</th>\n","      <td>1</td>\n","      <td>0.702284</td>\n","      <td>2.045574</td>\n","      <td>0.672676</td>\n","      <td>0.577953</td>\n","      <td>-0.840484</td>\n","      <td>-0.038680</td>\n","      <td>0.046588</td>\n","      <td>0.105777</td>\n","      <td>-0.809117</td>\n","      <td>...</td>\n","      <td>0.561361</td>\n","      <td>1.374854</td>\n","      <td>0.579001</td>\n","      <td>0.427906</td>\n","      <td>-0.809587</td>\n","      <td>0.350735</td>\n","      <td>0.326767</td>\n","      <td>0.414069</td>\n","      <td>-1.104549</td>\n","      <td>-0.318409</td>\n","    </tr>\n","    <tr>\n","      <th>567</th>\n","      <td>1</td>\n","      <td>1.838341</td>\n","      <td>2.336457</td>\n","      <td>1.982524</td>\n","      <td>1.735218</td>\n","      <td>1.525767</td>\n","      <td>3.272144</td>\n","      <td>3.296944</td>\n","      <td>2.658866</td>\n","      <td>2.137194</td>\n","      <td>...</td>\n","      <td>1.961239</td>\n","      <td>2.237926</td>\n","      <td>2.303601</td>\n","      <td>1.653171</td>\n","      <td>1.430427</td>\n","      <td>3.904848</td>\n","      <td>3.197605</td>\n","      <td>2.289985</td>\n","      <td>1.919083</td>\n","      <td>2.219635</td>\n","    </tr>\n","    <tr>\n","      <th>568</th>\n","      <td>0</td>\n","      <td>-1.808401</td>\n","      <td>1.221792</td>\n","      <td>-1.814389</td>\n","      <td>-1.347789</td>\n","      <td>-3.112085</td>\n","      <td>-1.150752</td>\n","      <td>-1.114873</td>\n","      <td>-1.261820</td>\n","      <td>-0.820070</td>\n","      <td>...</td>\n","      <td>-1.410893</td>\n","      <td>0.764190</td>\n","      <td>-1.432735</td>\n","      <td>-1.075813</td>\n","      <td>-1.859019</td>\n","      <td>-1.207552</td>\n","      <td>-1.305831</td>\n","      <td>-1.745063</td>\n","      <td>-0.048138</td>\n","      <td>-0.751207</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows √ó 31 columns</p>\n","</div>"],"text/plain":["     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","564          1     2.110995      0.721473        2.060786   2.343856   \n","565          1     1.704854      2.085134        1.615931   1.723842   \n","566          1     0.702284      2.045574        0.672676   0.577953   \n","567          1     1.838341      2.336457        1.982524   1.735218   \n","568          0    -1.808401      1.221792       -1.814389  -1.347789   \n","\n","     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","564         1.041842          0.219060        1.947285             2.320965   \n","565         0.102458         -0.017833        0.693043             1.263669   \n","566        -0.840484         -0.038680        0.046588             0.105777   \n","567         1.525767          3.272144        3.296944             2.658866   \n","568        -3.112085         -1.150752       -1.114873            -1.261820   \n","\n","     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n","564      -0.312589  ...      1.901185       0.117700         1.752563   \n","565      -0.217664  ...      1.536720       2.047399         1.421940   \n","566      -0.809117  ...      0.561361       1.374854         0.579001   \n","567       2.137194  ...      1.961239       2.237926         2.303601   \n","568      -0.820070  ...     -1.410893       0.764190        -1.432735   \n","\n","     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n","564    2.015301          0.378365          -0.273318         0.664512   \n","565    1.494959         -0.691230          -0.394820         0.236573   \n","566    0.427906         -0.809587           0.350735         0.326767   \n","567    1.653171          1.430427           3.904848         3.197605   \n","568   -1.075813         -1.859019          -1.207552        -1.305831   \n","\n","     concave points_worst  symmetry_worst  fractal_dimension_worst  \n","564              1.629151       -1.360158                -0.709091  \n","565              0.733827       -0.531855                -0.973978  \n","566              0.414069       -1.104549                -0.318409  \n","567              2.289985        1.919083                 2.219635  \n","568             -1.745063       -0.048138                -0.751207  \n","\n","[5 rows x 31 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["dataset.tail()"]},{"cell_type":"markdown","source":["üìå This time we have only one file, so we need to divide this dataset into a train set and test set ourselves. We can do that using the sklearn library function train test split."],"metadata":{"id":"_C5qZ4r2clU7"},"id":"_C5qZ4r2clU7"},{"cell_type":"code","execution_count":null,"id":"781db584","metadata":{"id":"781db584"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"id":"699bbe78","metadata":{"id":"699bbe78"},"outputs":[],"source":["train, test = train_test_split(dataset, test_size=0.3)"]},{"cell_type":"markdown","source":["üìå Also, as we did for the regression problem, we need to define the ‚Äútarget‚Äù we want to predict. In this problem, we‚Äôre trying to predict if the tumor is malignant (1) or benign (0). Hence, our target variable is the ‚Äúdiagnosis‚Äù column. And the rest of the columns are ‚Äúfeatures‚Äù. Let‚Äôs assign the x variable as target and the y variable as features. And remember, we need to do this for both the train and test datasets. By the way, you can also do this for the whole dataset and then divide into train and test."],"metadata":{"id":"v7dTQ6nrdIPJ"},"id":"v7dTQ6nrdIPJ"},{"cell_type":"code","execution_count":null,"id":"ae841721","metadata":{"id":"ae841721"},"outputs":[],"source":["X_train = train.drop(\"diagnosis\",axis=1)\n","y_train = train.loc[:,\"diagnosis\"]\n","\n","X_test = test.drop(\"diagnosis\",axis=1)\n","y_test = test.loc[:,\"diagnosis\"]"]},{"cell_type":"markdown","source":["üìå We‚Äôre ready to import our **logistic regression model** from **sklearn library**. And after importing the logistic regression model, we can assign it to the ‚Äúmodel‚Äù variable. Now, we‚Äôre ready to train our model, that means teach the hidden patterns in the train dataset to our model. And finally, we can make the predictions on the test dataset."],"metadata":{"id":"aO8BOavuc4GX"},"id":"aO8BOavuc4GX"},{"cell_type":"code","execution_count":null,"id":"e0f1935f","metadata":{"id":"e0f1935f"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","execution_count":null,"id":"86ab8163","metadata":{"id":"86ab8163"},"outputs":[],"source":["model_1 = LogisticRegression()"]},{"cell_type":"code","execution_count":null,"id":"744c283d","metadata":{"id":"744c283d","outputId":"b98b52cc-dfb4-47ac-be31-6f39a10f97dd"},"outputs":[{"data":{"text/plain":["LogisticRegression()"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["model_1.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"id":"7c79e90d","metadata":{"id":"7c79e90d","outputId":"3fa2dd35-196e-4774-ea4a-bb1d5c3ec04e"},"outputs":[{"data":{"text/plain":["array([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n","       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n","       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n","       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n","       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n","       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n","       1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0])"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["predictions = model_1.predict(X_test)\n","predictions"]},{"cell_type":"markdown","source":["üìå Using the **confusion matrix**, we can check the accuracy of our results. First, we import confusion_matrix from sklearn and display the number of each metric. We have 103 true negatives, 0 false positives, 4 false negatives, 64 true positives. That means out of 171 predictions are correct"],"metadata":{"id":"GCu9iwlsdiwc"},"id":"GCu9iwlsdiwc"},{"cell_type":"code","execution_count":null,"id":"a1840540","metadata":{"id":"a1840540","outputId":"8090f709-f040-4055-a014-dc2d8e76cc97"},"outputs":[{"data":{"text/plain":["array([[101,   0],\n","       [  6,  64]])"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import confusion_matrix\n","\n","confusion_matrix(y_test, predictions)"]},{"cell_type":"markdown","source":["üìå We import **classification_report** from **sklearn** and display the *evaluation metrics*. And the ratios we get, are quite high. We‚Äôve done a really good job."],"metadata":{"id":"rL1YQOp2d3MB"},"id":"rL1YQOp2d3MB"},{"cell_type":"code","execution_count":null,"id":"cbfed89a","metadata":{"id":"cbfed89a","outputId":"697d2ba3-2db9-4220-aeb2-f756e3193187"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      1.00      0.97       101\n","           1       1.00      0.91      0.96        70\n","\n","    accuracy                           0.96       171\n","   macro avg       0.97      0.96      0.96       171\n","weighted avg       0.97      0.96      0.96       171\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, predictions))"]},{"cell_type":"markdown","source":["üìå Well done! We have actually trained and tested a logistic regression classifier. Now, why not try another classification algorithm: **Support Vector Machine**. *SVM* is a **supervised machine learning technique** that can be used to solve classification and regression problems. It is, however, mostly used for classification. In this algorithm, we use an axis to represent each feature and plot all data points in the space. Then, the SVM model finds boundaries to separate these classes. The decision boundary is what separates different data samples into specific classes. Consider a dataset of different animals of two classes: birds and fish. In this dataset there are only three features: body weight, body length, and daily food consumption. We draw a 3-dimensional grid and plot all these points. A SVM model will try to find a 2D plane that differentiates the 2 classes.\n","\n","üìå If there were more than 3 features, we would have a hyper-space. A hyper-space is a space with higher than 3 dimensions like 4D, 5D, and so on and therefore it is not possible to visualize. We can find a hyper-plane that clearly distinguishes different classes. Hyper-planes are multidimensional planes that exist in four or more dimensions. This hyper-plane is used as a condition to perform classification. If the hyper-planes are linear, the SVM is called Linear Kernel SVM. However, the hyper-plane can be nonlinear as well. \n"],"metadata":{"id":"JJcjP9HUeAjd"},"id":"JJcjP9HUeAjd"},{"cell_type":"markdown","source":["üìå In that case we use a Polynomial Kernel or other advanced SVMs. Let‚Äôs see how this model performs with the same breast cancer dataset we used earlier. We start with importing LinearSVC from sklearn and assigning it to the variable. Now, we‚Äôre ready to train our model, that means teach the hidden patterns in the train dataset to our model. Finally, we can make the predictions on the test dataset."],"metadata":{"id":"QFbvS9fhep7S"},"id":"QFbvS9fhep7S"},{"cell_type":"code","execution_count":null,"id":"4502b446","metadata":{"id":"4502b446"},"outputs":[],"source":["from sklearn.svm import LinearSVC"]},{"cell_type":"code","execution_count":null,"id":"0c1962fc","metadata":{"id":"0c1962fc"},"outputs":[],"source":["model_2 = LinearSVC()"]},{"cell_type":"code","execution_count":null,"id":"d0e0aaa5","metadata":{"id":"d0e0aaa5","outputId":"475b2d21-9600-4b28-cdef-df3db503f666"},"outputs":[{"data":{"text/plain":["LinearSVC()"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["model_2.fit(X_train,y_train)"]},{"cell_type":"code","execution_count":null,"id":"06368e44","metadata":{"id":"06368e44","outputId":"b4e4cd80-09af-4a24-8527-ff7e75b79544"},"outputs":[{"data":{"text/plain":["array([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n","       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n","       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n","       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n","       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n","       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n","       1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["predictions = model_2.predict(X_test)\n","predictions"]},{"cell_type":"markdown","source":["üìå Our predictions with the Support Vector Classifier are ready! Now, we can check the accuracy of our model in the same way we did for Logistic Regression. We can start with the confusion matrix. We have one hundred one true negatives, 2 false positives, 4 false negatives, 64 true positives. This means that 165 out of 171 predictions are correct, just a couple less than before. "],"metadata":{"id":"UpU9GgLseyaO"},"id":"UpU9GgLseyaO"},{"cell_type":"code","execution_count":null,"id":"59a01cb3","metadata":{"id":"59a01cb3","outputId":"3757ea3e-60e8-438c-e606-4a3817d777f2"},"outputs":[{"data":{"text/plain":["array([[101,   0],\n","       [  7,  63]])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.metrics import confusion_matrix\n","\n","confusion_matrix(y_test, predictions)"]},{"cell_type":"markdown","source":["üìå We should also check the classification report. We get quite high metrics here as well. But we got better with Logistic Regression. And here our false positives were higher. This is a key metric for this dataset we want to minimize, because we don‚Äôt want healthy patients to be diagnosed with cancer. Therefore, we prefer using the Logistic Regression model for this problem and dataset."],"metadata":{"id":"YtdK2nj9fGpk"},"id":"YtdK2nj9fGpk"},{"cell_type":"code","execution_count":null,"id":"b699c1c0","metadata":{"id":"b699c1c0","outputId":"49740443-48b5-44ba-83b8-81295b5d37f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.94      1.00      0.97       101\n","           1       1.00      0.90      0.95        70\n","\n","    accuracy                           0.96       171\n","   macro avg       0.97      0.95      0.96       171\n","weighted avg       0.96      0.96      0.96       171\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","print(classification_report(y_test, predictions))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}