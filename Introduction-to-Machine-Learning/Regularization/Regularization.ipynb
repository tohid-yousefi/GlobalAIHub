{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Regularization - GlobalAIHub\n","\n","üìå We will start with overfitting and underfitting. A machine learning model can easily be **overfitted** or **underfitted** during training. In fact, these are some of the most common problems you can face while training your model. So, what exactly are underfitting and overfitting in ML? Let‚Äôs try to understand these concepts with a simple example. Consider a high school student, Clara. She has to prepare for a university entrance exam in biology. She is solving all the easy questions from her book. When she goes to take her exam, however, she will also see more complex questions. She won‚Äôt be able to solve them because she only trained on the easy ones and didn‚Äôt practice solving the more complex ones. This is underfitting.\n","\n","üìå  So, we could say that underfitting happens when a model is too simple, it is unable to find the patterns in the training data and therefore generates a high error on the training set, but also on unseen data. These models are also described as **‚Äúhighly biased‚Äù**. The bias refers to, the inability of the model to understand complexity of data. \n","\n","üìå  We usually get an underfit when there is not enough data in the training set, or it lacks complexity, meaning it has too few features to recognize patterns from. Let‚Äôs consider Clara once more. Now, she is solving math problems. Instead of studying for all the different types of math questions that will be covered in the exam, she is only focusing on algebra questions. Even though Clara is able to solve both easy and difficult algebra questions, when she sees geometry questions later in the exam, she is unable to solve them because she hasn‚Äôt studied them. This is overfitting.\n","\n","üìå We can think of overfitting as the opposite of underfitting. In this scenario, the model is trained too much on our specific training dataset and it generates high accuracy. But when it is applied to unseen data, the result has low accuracy. This is because it is looking for the patterns it has been trained on, but is unable to generalize in the test data. Generalization refers to the model‚Äôs ability to adapt to unseen data. These models are also described as, **‚Äúhigh variance models‚Äù**. The variance refers to the sensitivity of a model to specific datasets. More than it learns from the training data, it memorizes it.\n","\n","üìå We usually get an overfit, when the training data is very specific and has too many features. Both underfitting and overfitting lead to poor predictions. What we want to achieve is optimal fitting, a good balance. The performance of our model is affected by both **variance** and **bias**, which can lead to underfitting and overfitting. By adjusting variance and bias, we aim to generalize our model so that it is neither too complex nor too simple. Because as we have found out, overfitting with high bias or underfitting with high variance are not ideal for our model to make accurate predictions. By the way, we have to mention that there is a trade-off between bias and variance. This means that, as variance increases, bias decreases and vice versa.\n","\n","üìå Now that we have learned that high bias leads to underfitting, and high variance leads to overfitting, let‚Äôs discuss some approaches to solve these problems. We start with underfitting, as it is easier to deal with. The general approach to solving underfitting is to make the data more complex. We can increase the number of observations in the training set. We can also add new features that could impact the predictions. This is easy, because we don‚Äôt lose any original data from the training set, as we don‚Äôt remove anything. At the end, our model will gain more complexity and will try to find some patterns in the data that are closer to actual values.\n","\n","üìå Now, we can move on to overfitting. The general idea for solving overfitting and high variance is, to make the data less complex. Making data less complex is hard because by removing complexities, we may lose useful information that helps us to make predictions. One way to address this challenge is through regularization. **Regularization** prevents the learning of more complex patterns. It does this by shrinking coefficients towards zero, so that the impact of less significant features is reduced, and high variance is prevented. Regularization uses **loss functions that are called L1 and L2**. You are already familiar with one of the simplest and most common loss functions *‚ÄúMean Squared Error‚Äù**, MSE. You can think of the **L1** and **L2** loss functions as a modified version of that.\n","\n","üìå Generally, the L2 loss function is more common. But when there are outliers in the dataset, using the L2 loss function is not useful because, taking squares of the differences between the actual and predicted values will lead to a much larger error."],"metadata":{"id":"7q3ghxNIdlT2"}},{"cell_type":"markdown","source":["üìå Now, let‚Äôs see how these concepts play out in practice. In this practical example, we will try to make predictions using L2 regression. And then, we will show the results of regularization on accuracy, using the MSE metric. Let‚Äôs start with importing the required libraries. Now, we import the example dataset. Then we divide our dataset into features and targets. Finally, we split them into train and test datasets."],"metadata":{"id":"uy_7msKHg0HB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xibu6qo7pONV"},"outputs":[],"source":["import pandas as pd\n","from sklearn.linear_model import Ridge, LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error"]},{"cell_type":"code","source":["data = pd.read_csv(\"train.csv\")"],"metadata":{"id":"Tb6n-Ma_zXGj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = data.drop('SalePrice',axis=1)\n","y = data.loc[:,'SalePrice']"],"metadata":{"id":"UAp0ISkJzezQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"],"metadata":{"id":"HTqqAoiYrTyE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["üìå Let‚Äôs use ridge regression and compare the result to linear regression! We assign each of the two models to a variable in order to use it easily. You may notice that we used some hyperparameters, one is our lambda, which as explained before is our tuning parameter. The **‚Äúnormalize‚Äù** parameter converts all of the data points into the range of 0 and 1 which decreases the variety in our dataset. By setting different hyperparameters, we can improve our model."],"metadata":{"id":"FAp4Qw9Sg_fw"}},{"cell_type":"code","source":["linear_reg = LinearRegression()\n","ridge_reg = Ridge(alpha=0.05, normalize=True)"],"metadata":{"id":"C2l--kjHqty6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["üìå Then, we can train both of our models using the training dataset. Finally, we are ready to make predictions using the test dataset."],"metadata":{"id":"oB5ICePshQLe"}},{"cell_type":"code","source":["linear_reg.fit(X_train, y_train)\n","ridge_reg.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RG30YQkKq3rY","outputId":"ff4b43f7-c192-455d-c18c-bcc631c2cc99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n","If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n","\n","from sklearn.pipeline import make_pipeline\n","\n","model = make_pipeline(StandardScaler(with_mean=False), Ridge())\n","\n","If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n","\n","kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n","model.fit(X, y, **kwargs)\n","\n","Set parameter alpha to: original_alpha * n_samples. \n","  FutureWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["Ridge(alpha=0.05, normalize=True)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["linear_pred = linear_reg.predict(X_test)\n","ridge_pred = ridge_reg.predict(X_test)"],"metadata":{"id":"P_WIcc1wtDuu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["üìå Now, how can we compare the two models? Yes, we can use metrics! By calculating the mean squared error, **MSE**, we will be able to make a comparison between the performances of linear and ridge regression. Let‚Äôs print the results to see the difference. You can notice that, MSE for ridge regularization is lower, which means its predictions are better. Even though we only see a slight improvement in results, it won‚Äôt be the same for real-life datasets, since their size is larger and, they are more complex. The variety of data is greater and therefore, easier to overfit. Using regularization in such cases, will have a bigger impact on its accuracy."],"metadata":{"id":"6hjo32lihZnv"}},{"cell_type":"markdown","source":[],"metadata":{"id":"gaco8jcthZYs"}},{"cell_type":"code","source":["linear_mse = mean_squared_error(y_test, linear_pred)\n","ridge_mse = mean_squared_error(y_test, ridge_pred)"],"metadata":{"id":"ciejY2e4t7dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"MSE without Ridge: {linear_mse}\")\n","print(f\"MSE with Ridge : {ridge_mse}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7u6r3cgu3h5","outputId":"cec4d509-6b09-4aa0-d228-c3dacbe33ad8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MSE without Ridge: 1045213654.5698781\n","MSE with Ridge : 1036811749.9958531\n"]}]}]}